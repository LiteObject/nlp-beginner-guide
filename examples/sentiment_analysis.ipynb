{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f4b2ef7",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with Python\n",
    "\n",
    "This notebook demonstrates how to perform sentiment analysis using different approaches and libraries.\n",
    "\n",
    "## What you'll learn:\n",
    "- Rule-based sentiment analysis with TextBlob\n",
    "- Machine learning approach with scikit-learn\n",
    "- Using pre-trained models with transformers\n",
    "- Evaluating sentiment analysis results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdcc33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f08db1",
   "metadata": {},
   "source": [
    "## Sample Data\n",
    "Let's create some sample movie reviews for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1fab5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample movie reviews\n",
    "sample_reviews = [\n",
    "    \"This movie was absolutely fantastic! Great acting and amazing story.\",\n",
    "    \"Terrible film. Waste of time and money. Really disappointing.\",\n",
    "    \"Not bad, but could have been better. Average performance.\",\n",
    "    \"Outstanding cinematography and brilliant performances by all actors.\",\n",
    "    \"Boring and predictable. I fell asleep halfway through.\",\n",
    "    \"Loved every minute of it! Highly recommend to everyone.\",\n",
    "    \"The plot was confusing and the acting was poor.\",\n",
    "    \"A masterpiece! This will be remembered for years to come.\",\n",
    "    \"Okay movie, nothing special but watchable.\",\n",
    "    \"Worst movie I've ever seen. Completely awful.\"\n",
    "]\n",
    "\n",
    "# Expected sentiments (for comparison)\n",
    "expected_sentiments = ['positive', 'negative', 'neutral', 'positive', 'negative', \n",
    "                      'positive', 'negative', 'positive', 'neutral', 'negative']\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'review': sample_reviews,\n",
    "    'expected_sentiment': expected_sentiments\n",
    "})\n",
    "\n",
    "print(\"Sample reviews:\")\n",
    "for i, review in enumerate(sample_reviews):\n",
    "    print(f\"{i+1}. {review}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc347576",
   "metadata": {},
   "source": [
    "## Method 1: Rule-based Sentiment Analysis with TextBlob\n",
    "TextBlob provides a simple API for sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43285aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_textblob_sentiment(text):\n",
    "    \"\"\"\n",
    "    Get sentiment using TextBlob.\n",
    "    Returns polarity (-1 to 1) and subjectivity (0 to 1)\n",
    "    \"\"\"\n",
    "    blob = TextBlob(text)\n",
    "    return blob.sentiment.polarity, blob.sentiment.subjectivity\n",
    "\n",
    "def classify_sentiment(polarity):\n",
    "    \"\"\"\n",
    "    Classify sentiment based on polarity score.\n",
    "    \"\"\"\n",
    "    if polarity > 0.1:\n",
    "        return 'positive'\n",
    "    elif polarity < -0.1:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "# Apply TextBlob sentiment analysis\n",
    "df[['polarity', 'subjectivity']] = df['review'].apply(\n",
    "    lambda x: pd.Series(get_textblob_sentiment(x))\n",
    ")\n",
    "df['textblob_sentiment'] = df['polarity'].apply(classify_sentiment)\n",
    "\n",
    "print(\"TextBlob Sentiment Analysis Results:\")\n",
    "print(df[['review', 'polarity', 'subjectivity', 'textblob_sentiment', 'expected_sentiment']].to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e024ff4",
   "metadata": {},
   "source": [
    "## Visualizing Sentiment Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9463b42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Polarity distribution\n",
    "axes[0, 0].hist(df['polarity'], bins=10, color='skyblue', alpha=0.7)\n",
    "axes[0, 0].set_title('Distribution of Polarity Scores')\n",
    "axes[0, 0].set_xlabel('Polarity')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "\n",
    "# Subjectivity distribution\n",
    "axes[0, 1].hist(df['subjectivity'], bins=10, color='lightcoral', alpha=0.7)\n",
    "axes[0, 1].set_title('Distribution of Subjectivity Scores')\n",
    "axes[0, 1].set_xlabel('Subjectivity')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "\n",
    "# Sentiment comparison\n",
    "sentiment_comparison = pd.crosstab(df['expected_sentiment'], df['textblob_sentiment'])\n",
    "sns.heatmap(sentiment_comparison, annot=True, fmt='d', cmap='Blues', ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Expected vs TextBlob Sentiment')\n",
    "\n",
    "# Polarity vs Subjectivity scatter\n",
    "colors = {'positive': 'green', 'negative': 'red', 'neutral': 'gray'}\n",
    "for sentiment in df['expected_sentiment'].unique():\n",
    "    mask = df['expected_sentiment'] == sentiment\n",
    "    axes[1, 1].scatter(df[mask]['polarity'], df[mask]['subjectivity'], \n",
    "                      c=colors[sentiment], label=sentiment, alpha=0.7)\n",
    "axes[1, 1].set_xlabel('Polarity')\n",
    "axes[1, 1].set_ylabel('Subjectivity')\n",
    "axes[1, 1].set_title('Polarity vs Subjectivity')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b88e4c4",
   "metadata": {},
   "source": [
    "## Method 2: Machine Learning Approach\n",
    "Using TF-IDF features with Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08ae371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this example, let's create a larger synthetic dataset\n",
    "positive_samples = [\n",
    "    \"Excellent movie with great acting\",\n",
    "    \"Fantastic story and amazing visuals\",\n",
    "    \"Loved it! Highly recommended\",\n",
    "    \"Outstanding performance by all actors\",\n",
    "    \"Brilliant cinematography and direction\",\n",
    "    \"Best movie of the year\",\n",
    "    \"Wonderful experience, great entertainment\",\n",
    "    \"Perfect blend of action and emotion\",\n",
    "    \"Superb acting and engaging plot\",\n",
    "    \"Amazing special effects and sound\"\n",
    "]\n",
    "\n",
    "negative_samples = [\n",
    "    \"Terrible movie, waste of time\",\n",
    "    \"Boring and predictable plot\",\n",
    "    \"Poor acting and bad direction\",\n",
    "    \"Worst movie I've ever seen\",\n",
    "    \"Disappointing and confusing story\",\n",
    "    \"Awful script and terrible acting\",\n",
    "    \"Complete disaster, avoid at all costs\",\n",
    "    \"Poorly made with bad special effects\",\n",
    "    \"Dull and uninteresting characters\",\n",
    "    \"Failed to meet any expectations\"\n",
    "]\n",
    "\n",
    "neutral_samples = [\n",
    "    \"Average movie, nothing special\",\n",
    "    \"Okay film, could be better\",\n",
    "    \"Not bad but not great either\",\n",
    "    \"Decent enough for one viewing\",\n",
    "    \"Mixed feelings about this movie\",\n",
    "    \"Standard Hollywood production\",\n",
    "    \"Watchable but forgettable\",\n",
    "    \"Mediocre story with average acting\",\n",
    "    \"Neither good nor bad\",\n",
    "    \"Typical movie of this genre\"\n",
    "]\n",
    "\n",
    "# Create training dataset\n",
    "train_texts = positive_samples + negative_samples + neutral_samples\n",
    "train_labels = ['positive'] * len(positive_samples) + ['negative'] * len(negative_samples) + ['neutral'] * len(neutral_samples)\n",
    "\n",
    "print(f\"Training dataset: {len(train_texts)} samples\")\n",
    "print(f\"Positive: {len(positive_samples)}, Negative: {len(negative_samples)}, Neutral: {len(neutral_samples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4221c9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TF-IDF features\n",
    "vectorizer = TfidfVectorizer(max_features=1000, stop_words='english', lowercase=True)\n",
    "X = vectorizer.fit_transform(train_texts)\n",
    "y = train_labels\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# Train logistic regression model\n",
    "model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"Model Training Complete!\")\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Test set size: {X_test.shape[0]}\")\n",
    "print(f\"Number of features: {X_train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f83822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred, labels=['positive', 'negative', 'neutral'])\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['positive', 'negative', 'neutral'],\n",
    "            yticklabels=['positive', 'negative', 'neutral'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a68276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model on our original sample reviews\n",
    "sample_features = vectorizer.transform(df['review'])\n",
    "df['ml_sentiment'] = model.predict(sample_features)\n",
    "df['ml_confidence'] = model.predict_proba(sample_features).max(axis=1)\n",
    "\n",
    "print(\"ML Model Results on Sample Reviews:\")\n",
    "print(df[['review', 'expected_sentiment', 'textblob_sentiment', 'ml_sentiment', 'ml_confidence']].to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ede9bee",
   "metadata": {},
   "source": [
    "## Method 3: Using Pre-trained Transformers (Optional)\n",
    "This section uses the transformers library for state-of-the-art results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612fb8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This requires the transformers library to be installed\n",
    "# pip install transformers torch\n",
    "\n",
    "try:\n",
    "    from transformers import pipeline\n",
    "    \n",
    "    # Load pre-trained sentiment analysis pipeline\n",
    "    sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
    "    \n",
    "    # Test on a few examples\n",
    "    test_texts = df['review'].tolist()[:5]  # First 5 reviews\n",
    "    \n",
    "    results = sentiment_pipeline(test_texts)\n",
    "    \n",
    "    print(\"Transformer Model Results:\")\n",
    "    for text, result in zip(test_texts, results):\n",
    "        print(f\"Review: {text[:50]}...\")\n",
    "        print(f\"Sentiment: {result['label']}, Confidence: {result['score']:.3f}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"Transformers library not installed. To use this section, install with:\")\n",
    "    print(\"pip install transformers torch\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading transformer model: {e}\")\n",
    "    print(\"This might be due to internet connectivity or model loading issues.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2374bf",
   "metadata": {},
   "source": [
    "## Comparing All Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72088286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy for each method\n",
    "def calculate_accuracy(expected, predicted):\n",
    "    return sum(1 for e, p in zip(expected, predicted) if e == p) / len(expected)\n",
    "\n",
    "textblob_accuracy = calculate_accuracy(df['expected_sentiment'], df['textblob_sentiment'])\n",
    "ml_accuracy = calculate_accuracy(df['expected_sentiment'], df['ml_sentiment'])\n",
    "\n",
    "print(\"Method Comparison:\")\n",
    "print(f\"TextBlob Accuracy: {textblob_accuracy:.2f}\")\n",
    "print(f\"ML Model Accuracy: {ml_accuracy:.2f}\")\n",
    "\n",
    "# Visualization\n",
    "methods = ['TextBlob', 'ML Model']\n",
    "accuracies = [textblob_accuracy, ml_accuracy]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "bars = plt.bar(methods, accuracies, color=['skyblue', 'lightcoral'])\n",
    "plt.title('Sentiment Analysis Method Comparison')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "             f'{acc:.2f}', ha='center', va='bottom')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bb20a8",
   "metadata": {},
   "source": [
    "## Interactive Sentiment Analysis Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4d86cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment(text, method='all'):\n",
    "    \"\"\"\n",
    "    Analyze sentiment of given text using different methods.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Text to analyze\n",
    "        method (str): 'textblob', 'ml', or 'all'\n",
    "    \n",
    "    Returns:\n",
    "        dict: Results from different methods\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    if method in ['textblob', 'all']:\n",
    "        polarity, subjectivity = get_textblob_sentiment(text)\n",
    "        results['textblob'] = {\n",
    "            'sentiment': classify_sentiment(polarity),\n",
    "            'polarity': polarity,\n",
    "            'subjectivity': subjectivity\n",
    "        }\n",
    "    \n",
    "    if method in ['ml', 'all']:\n",
    "        text_features = vectorizer.transform([text])\n",
    "        prediction = model.predict(text_features)[0]\n",
    "        confidence = model.predict_proba(text_features).max()\n",
    "        results['ml'] = {\n",
    "            'sentiment': prediction,\n",
    "            'confidence': confidence\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test the function\n",
    "test_text = \"This is an amazing product! I love it so much!\"\n",
    "result = analyze_sentiment(test_text)\n",
    "\n",
    "print(f\"Text: {test_text}\")\n",
    "print(f\"TextBlob: {result['textblob']['sentiment']} (polarity: {result['textblob']['polarity']:.3f})\")\n",
    "print(f\"ML Model: {result['ml']['sentiment']} (confidence: {result['ml']['confidence']:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff21f0f9",
   "metadata": {},
   "source": [
    "## Practice Exercise\n",
    "\n",
    "Try analyzing sentiment for your own text examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeef3b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your own texts here and analyze their sentiment\n",
    "your_texts = [\n",
    "    \"I had a wonderful day at the beach!\",\n",
    "    \"The weather is so gloomy today.\",\n",
    "    \"This restaurant serves decent food.\"\n",
    "]\n",
    "\n",
    "print(\"Your Text Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for text in your_texts:\n",
    "    result = analyze_sentiment(text)\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"TextBlob: {result['textblob']['sentiment']} (polarity: {result['textblob']['polarity']:.3f})\")\n",
    "    print(f\"ML Model: {result['ml']['sentiment']} (confidence: {result['ml']['confidence']:.3f})\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777466f6",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **TextBlob** is simple and good for quick analysis, but may not be very accurate\n",
    "2. **Machine Learning** approaches require training data but can be more accurate\n",
    "3. **Pre-trained transformers** offer state-of-the-art performance but require more resources\n",
    "4. **Context matters** - the same words can have different sentiments in different contexts\n",
    "5. **Evaluation is important** - always test your sentiment analysis system on relevant data\n",
    "\n",
    "Choose the method that best fits your needs based on accuracy requirements, computational resources, and data availability."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
