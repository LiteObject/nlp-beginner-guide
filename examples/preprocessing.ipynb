{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8f31f78",
   "metadata": {},
   "source": [
    "# Text Preprocessing in NLP\n",
    "\n",
    "This notebook demonstrates essential text preprocessing techniques used in Natural Language Processing.\n",
    "\n",
    "## What you'll learn:\n",
    "- Tokenization\n",
    "- Lowercasing\n",
    "- Punctuation removal\n",
    "- Stop word removal\n",
    "- Stemming and Lemmatization\n",
    "- Complete preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57eaa81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f104c29",
   "metadata": {},
   "source": [
    "## Sample Text for Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e04cfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text for demonstration\n",
    "sample_text = \"\"\"Hello World! This is a SAMPLE text for demonstrating \n",
    "text preprocessing techniques in Natural Language Processing. \n",
    "It contains various punctuation marks, UPPERCASE letters, \n",
    "and common stopwords like 'the', 'and', 'is'. \n",
    "We'll also see how stemming and lemmatization work with words like \n",
    "'running', 'ran', 'better', and 'best'.\"\"\"\n",
    "\n",
    "print(\"Original text:\")\n",
    "print(sample_text)\n",
    "print(f\"\\nText length: {len(sample_text)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f54ec7",
   "metadata": {},
   "source": [
    "## Step 1: Tokenization\n",
    "Breaking text into individual words or sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6418dbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence tokenization\n",
    "sentences = sent_tokenize(sample_text)\n",
    "print(\"Sentences:\")\n",
    "for i, sentence in enumerate(sentences, 1):\n",
    "    print(f\"{i}. {sentence}\")\n",
    "\n",
    "print(f\"\\nNumber of sentences: {len(sentences)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9e5457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word tokenization\n",
    "tokens = word_tokenize(sample_text)\n",
    "print(\"Tokens:\")\n",
    "print(tokens)\n",
    "print(f\"\\nNumber of tokens: {len(tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc843e6",
   "metadata": {},
   "source": [
    "## Step 2: Convert to Lowercase\n",
    "Standardizing text case for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3add9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to lowercase\n",
    "lowercase_tokens = [token.lower() for token in tokens]\n",
    "print(\"Lowercase tokens:\")\n",
    "print(lowercase_tokens[:20])  # Show first 20 tokens\n",
    "\n",
    "# Compare before and after\n",
    "print(\"\\nBefore:\", tokens[:10])\n",
    "print(\"After:\", lowercase_tokens[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71db89f",
   "metadata": {},
   "source": [
    "## Step 3: Remove Punctuation\n",
    "Cleaning out punctuation marks that don't add meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa2d7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what punctuation looks like\n",
    "print(\"Punctuation marks:\", string.punctuation)\n",
    "\n",
    "# Remove punctuation\n",
    "no_punct_tokens = [token for token in lowercase_tokens if token not in string.punctuation]\n",
    "print(\"\\nTokens without punctuation:\")\n",
    "print(no_punct_tokens)\n",
    "\n",
    "print(f\"\\nTokens before: {len(lowercase_tokens)}\")\n",
    "print(f\"Tokens after: {len(no_punct_tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553c3857",
   "metadata": {},
   "source": [
    "## Step 4: Remove Stop Words\n",
    "Filtering out common words that don't carry much meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52800ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(\"Number of stopwords:\", len(stop_words))\n",
    "print(\"First 20 stopwords:\", list(stop_words)[:20])\n",
    "\n",
    "# Remove stopwords\n",
    "filtered_tokens = [token for token in no_punct_tokens if token not in stop_words]\n",
    "print(\"\\nTokens after removing stopwords:\")\n",
    "print(filtered_tokens)\n",
    "\n",
    "print(f\"\\nTokens before: {len(no_punct_tokens)}\")\n",
    "print(f\"Tokens after: {len(filtered_tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d964f88c",
   "metadata": {},
   "source": [
    "## Step 5: Stemming\n",
    "Reducing words to their root form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366eb5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Apply stemming\n",
    "stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
    "print(\"Stemmed tokens:\")\n",
    "print(stemmed_tokens)\n",
    "\n",
    "# Show some examples of stemming\n",
    "example_words = ['running', 'ran', 'runs', 'better', 'best', 'preprocessing', 'demonstrates']\n",
    "print(\"\\nStemming examples:\")\n",
    "for word in example_words:\n",
    "    print(f\"{word} -> {stemmer.stem(word)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cc23d6",
   "metadata": {},
   "source": [
    "## Step 6: Lemmatization\n",
    "Converting words to their base or dictionary form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4b449b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Apply lemmatization\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "print(\"Lemmatized tokens:\")\n",
    "print(lemmatized_tokens)\n",
    "\n",
    "# Compare stemming vs lemmatization\n",
    "print(\"\\nStemming vs Lemmatization:\")\n",
    "for word in example_words:\n",
    "    print(f\"{word} -> Stem: {stemmer.stem(word)}, Lemma: {lemmatizer.lemmatize(word)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78c91ee",
   "metadata": {},
   "source": [
    "## Complete Preprocessing Function\n",
    "Putting it all together in a reusable function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba49b69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, remove_stopwords=True, use_stemming=False, use_lemmatization=True):\n",
    "    \"\"\"\n",
    "    Complete text preprocessing pipeline.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Raw text to preprocess\n",
    "        remove_stopwords (bool): Whether to remove stopwords\n",
    "        use_stemming (bool): Whether to apply stemming\n",
    "        use_lemmatization (bool): Whether to apply lemmatization\n",
    "    \n",
    "    Returns:\n",
    "        list: Preprocessed tokens\n",
    "    \"\"\"\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    \n",
    "    # Remove punctuation and non-alphabetic tokens\n",
    "    tokens = [token for token in tokens if token.isalpha()]\n",
    "    \n",
    "    # Remove stopwords\n",
    "    if remove_stopwords:\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Apply stemming\n",
    "    if use_stemming:\n",
    "        stemmer = PorterStemmer()\n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "    \n",
    "    # Apply lemmatization\n",
    "    if use_lemmatization:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Test the function\n",
    "processed = preprocess_text(sample_text)\n",
    "print(\"Final preprocessed tokens:\")\n",
    "print(processed)\n",
    "print(f\"\\nOriginal tokens: {len(word_tokenize(sample_text))}\")\n",
    "print(f\"Processed tokens: {len(processed)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e070df0d",
   "metadata": {},
   "source": [
    "## Comparing Different Preprocessing Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168957b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different preprocessing options\n",
    "print(\"Comparison of preprocessing options:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Basic preprocessing\n",
    "basic = preprocess_text(sample_text, remove_stopwords=False, use_stemming=False, use_lemmatization=False)\n",
    "print(f\"Basic (lowercase, no punctuation): {len(basic)} tokens\")\n",
    "print(basic[:10])\n",
    "\n",
    "# With stopword removal\n",
    "no_stopwords = preprocess_text(sample_text, remove_stopwords=True, use_stemming=False, use_lemmatization=False)\n",
    "print(f\"\\nWithout stopwords: {len(no_stopwords)} tokens\")\n",
    "print(no_stopwords[:10])\n",
    "\n",
    "# With stemming\n",
    "with_stemming = preprocess_text(sample_text, remove_stopwords=True, use_stemming=True, use_lemmatization=False)\n",
    "print(f\"\\nWith stemming: {len(with_stemming)} tokens\")\n",
    "print(with_stemming[:10])\n",
    "\n",
    "# With lemmatization (default)\n",
    "with_lemmatization = preprocess_text(sample_text)\n",
    "print(f\"\\nWith lemmatization: {len(with_lemmatization)} tokens\")\n",
    "print(with_lemmatization[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bf6848",
   "metadata": {},
   "source": [
    "## Practice Exercise\n",
    "\n",
    "Try preprocessing this text with different options and observe the differences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fd5814",
   "metadata": {},
   "outputs": [],
   "source": [
    "practice_text = \"\"\"\n",
    "The runners were running quickly through the beautiful gardens. \n",
    "They had been training for months, and their performance was better than expected. \n",
    "\"This is the best race I've ever run!\" shouted one of the participants.\n",
    "\"\"\"\n",
    "\n",
    "# Your turn: try different preprocessing options on this text\n",
    "# TODO: Experiment with the preprocessing function\n",
    "\n",
    "result = preprocess_text(practice_text)\n",
    "print(\"Preprocessed practice text:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254e042c",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Tokenization** breaks text into meaningful units\n",
    "2. **Lowercasing** ensures consistency\n",
    "3. **Punctuation removal** cleans the text\n",
    "4. **Stopword removal** focuses on meaningful words\n",
    "5. **Stemming** is fast but sometimes inaccurate\n",
    "6. **Lemmatization** is more accurate but slower\n",
    "\n",
    "The choice of preprocessing techniques depends on your specific use case and the trade-offs you're willing to make between accuracy and performance."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
