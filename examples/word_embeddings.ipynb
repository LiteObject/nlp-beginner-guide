{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e419bd8",
   "metadata": {},
   "source": [
    "# Word Embeddings in NLP\n",
    "\n",
    "This notebook explores different types of word embeddings and their applications.\n",
    "\n",
    "## What you'll learn:\n",
    "- One-hot encoding vs. dense embeddings\n",
    "- Word2Vec (CBOW and Skip-gram)\n",
    "- GloVe embeddings\n",
    "- Using pre-trained embeddings\n",
    "- Visualizing word relationships\n",
    "- Word similarity and analogies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896ab2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a24c412",
   "metadata": {},
   "source": [
    "## Understanding Word Representations\n",
    "Let's start by understanding different ways to represent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c403a74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample vocabulary\n",
    "vocabulary = ['king', 'queen', 'man', 'woman', 'boy', 'girl', 'prince', 'princess']\n",
    "vocab_size = len(vocabulary)\n",
    "\n",
    "print(f\"Vocabulary: {vocabulary}\")\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "\n",
    "# Create word to index mapping\n",
    "word_to_idx = {word: i for i, word in enumerate(vocabulary)}\n",
    "idx_to_word = {i: word for i, word in enumerate(vocabulary)}\n",
    "\n",
    "print(f\"\\nWord to index mapping: {word_to_idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928bf0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding representation\n",
    "def create_one_hot(word, word_to_idx, vocab_size):\n",
    "    \"\"\"\n",
    "    Create one-hot vector for a word.\n",
    "    \"\"\"\n",
    "    vector = np.zeros(vocab_size)\n",
    "    if word in word_to_idx:\n",
    "        vector[word_to_idx[word]] = 1\n",
    "    return vector\n",
    "\n",
    "# Example one-hot vectors\n",
    "print(\"One-hot Encoding Examples:\")\n",
    "for word in ['king', 'queen', 'man']:\n",
    "    one_hot = create_one_hot(word, word_to_idx, vocab_size)\n",
    "    print(f\"{word}: {one_hot}\")\n",
    "\n",
    "# Problems with one-hot encoding\n",
    "king_vec = create_one_hot('king', word_to_idx, vocab_size)\n",
    "queen_vec = create_one_hot('queen', word_to_idx, vocab_size)\n",
    "man_vec = create_one_hot('man', word_to_idx, vocab_size)\n",
    "\n",
    "# Calculate cosine similarity\n",
    "king_queen_sim = cosine_similarity([king_vec], [queen_vec])[0][0]\n",
    "king_man_sim = cosine_similarity([king_vec], [man_vec])[0][0]\n",
    "\n",
    "print(f\"\\nCosine similarity (one-hot):\")\n",
    "print(f\"King-Queen: {king_queen_sim:.3f}\")\n",
    "print(f\"King-Man: {king_man_sim:.3f}\")\n",
    "print(\"Problem: All words are equally dissimilar!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0f3a9f",
   "metadata": {},
   "source": [
    "## Creating Sample Corpus\n",
    "We'll create a sample corpus to train our word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8aeac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample sentences about royalty and family\n",
    "sample_sentences = [\n",
    "    \"the king rules the kingdom with wisdom\",\n",
    "    \"the queen is elegant and powerful\",\n",
    "    \"a man works hard every day\",\n",
    "    \"a woman leads with grace and strength\",\n",
    "    \"the boy plays in the garden\",\n",
    "    \"the girl studies books diligently\",\n",
    "    \"the prince will become king someday\",\n",
    "    \"the princess is kind and smart\",\n",
    "    \"the king and queen rule together\",\n",
    "    \"the man and woman are married\",\n",
    "    \"the boy and girl are siblings\",\n",
    "    \"the prince and princess are royal\",\n",
    "    \"a wise king makes good decisions\",\n",
    "    \"a strong queen protects her people\",\n",
    "    \"the young man works in the city\",\n",
    "    \"the young woman teaches children\",\n",
    "    \"the little boy loves to play\",\n",
    "    \"the little girl enjoys reading\",\n",
    "    \"the brave prince fights dragons\",\n",
    "    \"the beautiful princess sings songs\"\n",
    "]\n",
    "\n",
    "# Tokenize sentences\n",
    "tokenized_sentences = [sentence.split() for sentence in sample_sentences]\n",
    "\n",
    "print(\"Sample corpus:\")\n",
    "for i, sentence in enumerate(tokenized_sentences[:5]):\n",
    "    print(f\"{i+1}. {sentence}\")\n",
    "print(f\"... and {len(tokenized_sentences)-5} more sentences\")\n",
    "\n",
    "print(f\"\\nTotal sentences: {len(tokenized_sentences)}\")\n",
    "print(f\"Total words: {sum(len(sentence) for sentence in tokenized_sentences)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47be17e7",
   "metadata": {},
   "source": [
    "## Training Word2Vec Model\n",
    "Word2Vec learns dense vector representations that capture semantic relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72c6aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Word2Vec model\n",
    "model = Word2Vec(\n",
    "    sentences=tokenized_sentences,\n",
    "    vector_size=50,  # Dimension of embeddings\n",
    "    window=3,        # Context window size\n",
    "    min_count=1,     # Minimum word frequency\n",
    "    workers=1,       # Number of threads\n",
    "    sg=0            # 0 for CBOW, 1 for Skip-gram\n",
    ")\n",
    "\n",
    "print(\"Word2Vec model trained successfully!\")\n",
    "print(f\"Vocabulary size: {len(model.wv.key_to_index)}\")\n",
    "print(f\"Vector dimensions: {model.wv.vector_size}\")\n",
    "print(f\"Vocabulary: {list(model.wv.key_to_index.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5292bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get word vectors\n",
    "def get_word_vector(word, model):\n",
    "    \"\"\"Get vector for a word if it exists in vocabulary\"\"\"\n",
    "    if word in model.wv.key_to_index:\n",
    "        return model.wv[word]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Example word vectors\n",
    "king_vector = get_word_vector('king', model)\n",
    "queen_vector = get_word_vector('queen', model)\n",
    "\n",
    "print(\"Word2Vec Embeddings:\")\n",
    "print(f\"King vector (first 10 dims): {king_vector[:10]}\")\n",
    "print(f\"Queen vector (first 10 dims): {queen_vector[:10]}\")\n",
    "print(f\"Vector shape: {king_vector.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405e88af",
   "metadata": {},
   "source": [
    "## Exploring Word Similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06718adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate similarities between words\n",
    "def calculate_similarity(word1, word2, model):\n",
    "    \"\"\"Calculate cosine similarity between two words\"\"\"\n",
    "    try:\n",
    "        return model.wv.similarity(word1, word2)\n",
    "    except KeyError:\n",
    "        return 0.0\n",
    "\n",
    "# Word pairs to compare\n",
    "word_pairs = [\n",
    "    ('king', 'queen'),\n",
    "    ('king', 'prince'),\n",
    "    ('queen', 'princess'),\n",
    "    ('man', 'woman'),\n",
    "    ('boy', 'girl'),\n",
    "    ('king', 'man'),\n",
    "    ('queen', 'woman'),\n",
    "    ('prince', 'boy'),\n",
    "    ('princess', 'girl')\n",
    "]\n",
    "\n",
    "print(\"Word Similarities (Word2Vec):\")\n",
    "print(\"=\" * 40)\n",
    "for word1, word2 in word_pairs:\n",
    "    similarity = calculate_similarity(word1, word2, model)\n",
    "    print(f\"{word1:>8} - {word2:<8}: {similarity:.3f}\")\n",
    "\n",
    "# Find most similar words\n",
    "print(\"\\nMost similar words:\")\n",
    "for word in ['king', 'queen', 'man', 'woman']:\n",
    "    if word in model.wv.key_to_index:\n",
    "        similar = model.wv.most_similar(word, topn=3)\n",
    "        print(f\"{word}: {similar}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad78293f",
   "metadata": {},
   "source": [
    "## Word Analogies\n",
    "Testing the famous \"king - man + woman = queen\" analogy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1229226f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test word analogies\n",
    "def test_analogy(word1, word2, word3, model, topn=3):\n",
    "    \"\"\"\n",
    "    Test analogy: word1 is to word2 as word3 is to ?\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = model.wv.most_similar(positive=[word2, word3], negative=[word1], topn=topn)\n",
    "        return result\n",
    "    except KeyError as e:\n",
    "        return f\"Word not in vocabulary: {e}\"\n",
    "\n",
    "# Test analogies\n",
    "analogies = [\n",
    "    ('king', 'man', 'queen'),      # king - man + queen = ?\n",
    "    ('man', 'king', 'woman'),      # man - king + woman = ?\n",
    "    ('prince', 'boy', 'princess'), # prince - boy + princess = ?\n",
    "    ('boy', 'prince', 'girl')      # boy - prince + girl = ?\n",
    "]\n",
    "\n",
    "print(\"Word Analogies:\")\n",
    "print(\"=\" * 50)\n",
    "for word1, word2, word3 in analogies:\n",
    "    result = test_analogy(word1, word2, word3, model)\n",
    "    print(f\"{word1} - {word2} + {word3} = {result}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb38de43",
   "metadata": {},
   "source": [
    "## Visualizing Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79165519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all word vectors\n",
    "words = list(model.wv.key_to_index.keys())\n",
    "vectors = [model.wv[word] for word in words]\n",
    "\n",
    "# Reduce dimensionality using PCA\n",
    "pca = PCA(n_components=2)\n",
    "vectors_2d = pca.fit_transform(vectors)\n",
    "\n",
    "# Create DataFrame for plotting\n",
    "df_plot = pd.DataFrame({\n",
    "    'word': words,\n",
    "    'x': vectors_2d[:, 0],\n",
    "    'y': vectors_2d[:, 1]\n",
    "})\n",
    "\n",
    "# Plot embeddings\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(df_plot['x'], df_plot['y'], alpha=0.7, s=100)\n",
    "\n",
    "# Add word labels\n",
    "for _, row in df_plot.iterrows():\n",
    "    plt.annotate(row['word'], (row['x'], row['y']), \n",
    "                xytext=(5, 5), textcoords='offset points', fontsize=10)\n",
    "\n",
    "plt.title('Word Embeddings Visualization (PCA)')\n",
    "plt.xlabel('First Principal Component')\n",
    "plt.ylabel('Second Principal Component')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Explained variance ratio: {pca.explained_variance_ratio_}\")\n",
    "print(f\"Total explained variance: {sum(pca.explained_variance_ratio_):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb9a813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative visualization with t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=5)\n",
    "vectors_tsne = tsne.fit_transform(vectors)\n",
    "\n",
    "df_tsne = pd.DataFrame({\n",
    "    'word': words,\n",
    "    'x': vectors_tsne[:, 0],\n",
    "    'y': vectors_tsne[:, 1]\n",
    "})\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(df_tsne['x'], df_tsne['y'], alpha=0.7, s=100, c='red')\n",
    "\n",
    "for _, row in df_tsne.iterrows():\n",
    "    plt.annotate(row['word'], (row['x'], row['y']), \n",
    "                xytext=(5, 5), textcoords='offset points', fontsize=10)\n",
    "\n",
    "plt.title('Word Embeddings Visualization (t-SNE)')\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d324123",
   "metadata": {},
   "source": [
    "## Comparing CBOW vs Skip-gram\n",
    "Let's train both architectures and compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46e62a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train CBOW model (sg=0)\n",
    "cbow_model = Word2Vec(\n",
    "    sentences=tokenized_sentences,\n",
    "    vector_size=50,\n",
    "    window=3,\n",
    "    min_count=1,\n",
    "    workers=1,\n",
    "    sg=0  # CBOW\n",
    ")\n",
    "\n",
    "# Train Skip-gram model (sg=1)\n",
    "skipgram_model = Word2Vec(\n",
    "    sentences=tokenized_sentences,\n",
    "    vector_size=50,\n",
    "    window=3,\n",
    "    min_count=1,\n",
    "    workers=1,\n",
    "    sg=1  # Skip-gram\n",
    ")\n",
    "\n",
    "print(\"Both models trained successfully!\")\n",
    "\n",
    "# Compare similarities\n",
    "test_pairs = [('king', 'queen'), ('man', 'woman'), ('boy', 'girl')]\n",
    "\n",
    "print(\"\\nComparison of CBOW vs Skip-gram:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"{'Word Pair':<15} {'CBOW':<10} {'Skip-gram':<10}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for word1, word2 in test_pairs:\n",
    "    cbow_sim = calculate_similarity(word1, word2, cbow_model)\n",
    "    skipgram_sim = calculate_similarity(word1, word2, skipgram_model)\n",
    "    print(f\"{word1}-{word2:<10} {cbow_sim:<10.3f} {skipgram_sim:<10.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafe0ae6",
   "metadata": {},
   "source": [
    "## Using Pre-trained Embeddings (Demo)\n",
    "This section shows how to load and use pre-trained embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338ce7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This would normally load pre-trained embeddings\n",
    "# For demonstration, we'll show the process\n",
    "\n",
    "print(\"Loading Pre-trained Embeddings (Demo):\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Example of how to load Google's Word2Vec model\n",
    "print(\"To load Google's pre-trained Word2Vec model:\")\n",
    "print(\"1. Download: GoogleNews-vectors-negative300.bin.gz\")\n",
    "print(\"2. Code: model = KeyedVectors.load_word2vec_format('path/to/file', binary=True)\")\n",
    "print(\"3. Usage: model['word'] or model.similarity('word1', 'word2')\")\n",
    "\n",
    "print(\"\\nTo load GloVe embeddings:\")\n",
    "print(\"1. Download: glove.6B.300d.txt\")\n",
    "print(\"2. Code: model = KeyedVectors.load_word2vec_format('path/to/file', binary=False)\")\n",
    "\n",
    "print(\"\\nAdvantages of pre-trained embeddings:\")\n",
    "print(\"- Trained on large corpora (billions of words)\")\n",
    "print(\"- Capture general semantic relationships\")\n",
    "print(\"- Ready to use without training time\")\n",
    "print(\"- Often work better than custom embeddings on small datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb323b5",
   "metadata": {},
   "source": [
    "## Creating a Simple Word Embedding Lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72783a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleEmbeddingLookup:\n",
    "    \"\"\"\n",
    "    Simple class to handle word embeddings lookup and operations.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.vocab = set(model.wv.key_to_index.keys())\n",
    "    \n",
    "    def get_vector(self, word):\n",
    "        \"\"\"Get vector for a word\"\"\"\n",
    "        if word in self.vocab:\n",
    "            return self.model.wv[word]\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def similarity(self, word1, word2):\n",
    "        \"\"\"Calculate similarity between two words\"\"\"\n",
    "        try:\n",
    "            return self.model.wv.similarity(word1, word2)\n",
    "        except KeyError:\n",
    "            return 0.0\n",
    "    \n",
    "    def most_similar(self, word, topn=5):\n",
    "        \"\"\"Find most similar words\"\"\"\n",
    "        if word in self.vocab:\n",
    "            return self.model.wv.most_similar(word, topn=topn)\n",
    "        else:\n",
    "            return []\n",
    "    \n",
    "    def analogy(self, word1, word2, word3, topn=3):\n",
    "        \"\"\"Solve word analogy: word1 is to word2 as word3 is to ?\"\"\"\n",
    "        try:\n",
    "            return self.model.wv.most_similar(positive=[word2, word3], negative=[word1], topn=topn)\n",
    "        except KeyError:\n",
    "            return []\n",
    "    \n",
    "    def word_in_vocab(self, word):\n",
    "        \"\"\"Check if word is in vocabulary\"\"\"\n",
    "        return word in self.vocab\n",
    "\n",
    "# Create embedding lookup instance\n",
    "embedding_lookup = SimpleEmbeddingLookup(model)\n",
    "\n",
    "# Test the lookup\n",
    "print(\"Testing Embedding Lookup:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Test similarity\n",
    "sim = embedding_lookup.similarity('king', 'queen')\n",
    "print(f\"King-Queen similarity: {sim:.3f}\")\n",
    "\n",
    "# Test most similar\n",
    "similar = embedding_lookup.most_similar('king')\n",
    "print(f\"Most similar to 'king': {similar}\")\n",
    "\n",
    "# Test analogy\n",
    "analogy_result = embedding_lookup.analogy('king', 'man', 'queen')\n",
    "print(f\"King - man + queen = {analogy_result}\")\n",
    "\n",
    "# Test vocabulary check\n",
    "print(f\"'king' in vocabulary: {embedding_lookup.word_in_vocab('king')}\")\n",
    "print(f\"'elephant' in vocabulary: {embedding_lookup.word_in_vocab('elephant')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bbf5af",
   "metadata": {},
   "source": [
    "## Practical Applications\n",
    "Examples of how to use word embeddings in real applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a7a118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application 1: Document Similarity using average word embeddings\n",
    "def document_vector(doc, model):\n",
    "    \"\"\"\n",
    "    Create document vector by averaging word embeddings.\n",
    "    \"\"\"\n",
    "    words = doc.split()\n",
    "    word_vectors = []\n",
    "    \n",
    "    for word in words:\n",
    "        if word in model.wv.key_to_index:\n",
    "            word_vectors.append(model.wv[word])\n",
    "    \n",
    "    if word_vectors:\n",
    "        return np.mean(word_vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.wv.vector_size)\n",
    "\n",
    "# Test documents\n",
    "docs = [\n",
    "    \"the king rules the kingdom\",\n",
    "    \"the queen is very wise\",\n",
    "    \"the man works hard\",\n",
    "    \"the woman is strong\"\n",
    "]\n",
    "\n",
    "# Create document vectors\n",
    "doc_vectors = [document_vector(doc, model) for doc in docs]\n",
    "\n",
    "# Calculate document similarities\n",
    "print(\"Document Similarity Matrix:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "similarity_matrix = cosine_similarity(doc_vectors)\n",
    "\n",
    "for i, doc1 in enumerate(docs):\n",
    "    for j, doc2 in enumerate(docs):\n",
    "        if i < j:  # Only show upper triangle\n",
    "            sim = similarity_matrix[i][j]\n",
    "            print(f\"Doc {i+1} vs Doc {j+1}: {sim:.3f}\")\n",
    "            print(f\"  '{doc1}' vs '{doc2}'\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3dd7009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application 2: Word clustering based on embeddings\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Get vectors for clustering\n",
    "words_to_cluster = ['king', 'queen', 'prince', 'princess', 'man', 'woman', 'boy', 'girl']\n",
    "vectors_to_cluster = [model.wv[word] for word in words_to_cluster if word in model.wv.key_to_index]\n",
    "valid_words = [word for word in words_to_cluster if word in model.wv.key_to_index]\n",
    "\n",
    "# Perform k-means clustering\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "clusters = kmeans.fit_predict(vectors_to_cluster)\n",
    "\n",
    "# Show clustering results\n",
    "print(\"Word Clustering Results:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "cluster_dict = {}\n",
    "for word, cluster in zip(valid_words, clusters):\n",
    "    if cluster not in cluster_dict:\n",
    "        cluster_dict[cluster] = []\n",
    "    cluster_dict[cluster].append(word)\n",
    "\n",
    "for cluster_id, words in cluster_dict.items():\n",
    "    print(f\"Cluster {cluster_id}: {words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e55a58b",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Dense representations** capture semantic relationships better than one-hot encoding\n",
    "2. **Word2Vec** learns embeddings by predicting context (CBOW) or target words (Skip-gram)\n",
    "3. **Similar words** have similar vector representations\n",
    "4. **Analogies** can be solved using vector arithmetic\n",
    "5. **Pre-trained embeddings** often work better than training from scratch\n",
    "6. **Applications** include document similarity, clustering, and as features for ML models\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Explore FastText embeddings (subword information)\n",
    "- Try contextualized embeddings (BERT, ELMo)\n",
    "- Use embeddings as features in downstream tasks\n",
    "- Experiment with different embedding dimensions\n",
    "- Learn about sentence and document embeddings"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
