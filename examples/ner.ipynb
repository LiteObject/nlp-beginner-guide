{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "962994b4",
   "metadata": {},
   "source": [
    "# Named Entity Recognition (NER)\n",
    "\n",
    "This notebook demonstrates Named Entity Recognition using different approaches and tools.\n",
    "\n",
    "## What you'll learn:\n",
    "- What is Named Entity Recognition\n",
    "- Using spaCy for NER\n",
    "- Custom NER with machine learning\n",
    "- Evaluating NER models\n",
    "- Visualizing NER results\n",
    "- Building a simple NER system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd724be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter, defaultdict\n",
    "import re\n",
    "\n",
    "# Try to import spaCy (if available)\n",
    "try:\n",
    "    import spacy\n",
    "    from spacy import displacy\n",
    "    SPACY_AVAILABLE = True\n",
    "    print(\"spaCy imported successfully!\")\n",
    "except ImportError:\n",
    "    SPACY_AVAILABLE = False\n",
    "    print(\"spaCy not available. Install with: pip install spacy\")\n",
    "    print(\"Then download a model: python -m spacy download en_core_web_sm\")\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b1e60b",
   "metadata": {},
   "source": [
    "## Understanding Named Entity Recognition\n",
    "NER identifies and classifies named entities in text into predefined categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1eb49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample texts with various named entities\n",
    "sample_texts = [\n",
    "    \"Apple Inc. is planning to open a new store in New York City next month.\",\n",
    "    \"Microsoft Corporation was founded by Bill Gates and Paul Allen in 1975.\",\n",
    "    \"The Amazon rainforest covers much of the Amazon basin in South America.\",\n",
    "    \"Google's headquarters is located in Mountain View, California.\",\n",
    "    \"The iPhone was first released by Apple on June 29, 2007.\",\n",
    "    \"Elon Musk is the CEO of Tesla and SpaceX companies.\"\n",
    "]\n",
    "\n",
    "print(\"Sample texts for NER analysis:\")\n",
    "for i, text in enumerate(sample_texts, 1):\n",
    "    print(f\"{i}. {text}\")\n",
    "\n",
    "print(\"\\nCommon entity types:\")\n",
    "entity_types = {\n",
    "    'PERSON': 'People, including fictional characters',\n",
    "    'ORG': 'Organizations, companies, agencies',\n",
    "    'GPE': 'Geopolitical entities (countries, cities, states)',\n",
    "    'DATE': 'Dates or periods',\n",
    "    'MONEY': 'Monetary values',\n",
    "    'PRODUCT': 'Products, vehicles, foods, etc.',\n",
    "    'EVENT': 'Named events',\n",
    "    'LOC': 'Locations, mountain ranges, bodies of water'\n",
    "}\n",
    "\n",
    "for entity, description in entity_types.items():\n",
    "    print(f\"{entity:>8}: {description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b92a4b",
   "metadata": {},
   "source": [
    "## Using spaCy for NER\n",
    "spaCy provides pre-trained NER models that work out of the box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42aaa155",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SPACY_AVAILABLE:\n",
    "    try:\n",
    "        # Load English model\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "        \n",
    "        def extract_entities_spacy(text):\n",
    "            \"\"\"Extract named entities using spaCy\"\"\"\n",
    "            doc = nlp(text)\n",
    "            entities = []\n",
    "            for ent in doc.ents:\n",
    "                entities.append({\n",
    "                    'text': ent.text,\n",
    "                    'label': ent.label_,\n",
    "                    'start': ent.start_char,\n",
    "                    'end': ent.end_char,\n",
    "                    'description': spacy.explain(ent.label_)\n",
    "                })\n",
    "            return entities\n",
    "        \n",
    "        # Test on sample texts\n",
    "        print(\"spaCy NER Results:\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        for i, text in enumerate(sample_texts[:3], 1):\n",
    "            print(f\"\\nText {i}: {text}\")\n",
    "            entities = extract_entities_spacy(text)\n",
    "            \n",
    "            if entities:\n",
    "                print(\"Entities found:\")\n",
    "                for ent in entities:\n",
    "                    print(f\"  {ent['text']:>15} -> {ent['label']:<8} ({ent['description']})\")\n",
    "            else:\n",
    "                print(\"  No entities found.\")\n",
    "            print(\"-\" * 50)\n",
    "        \n",
    "        SPACY_MODEL_LOADED = True\n",
    "        \n",
    "    except OSError:\n",
    "        print(\"spaCy model 'en_core_web_sm' not found.\")\n",
    "        print(\"Download it with: python -m spacy download en_core_web_sm\")\n",
    "        SPACY_MODEL_LOADED = False\n",
    "else:\n",
    "    print(\"spaCy not available. Proceeding with rule-based approach.\")\n",
    "    SPACY_MODEL_LOADED = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a41ad2",
   "metadata": {},
   "source": [
    "## Rule-based NER Approach\n",
    "Creating a simple rule-based system for entity recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40c1eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNER:\n",
    "    \"\"\"\n",
    "    A simple rule-based Named Entity Recognition system.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Define patterns for different entity types\n",
    "        self.patterns = {\n",
    "            'PERSON': [\n",
    "                r'\\b[A-Z][a-z]+ [A-Z][a-z]+\\b',  # First Last\n",
    "                r'\\b(?:Mr|Mrs|Ms|Dr|Prof)\\. [A-Z][a-z]+\\b'  # Title + Name\n",
    "            ],\n",
    "            'ORG': [\n",
    "                r'\\b[A-Z][a-z]+ (?:Inc|Corp|Corporation|LLC|Ltd|Company)\\b',\n",
    "                r'\\b(?:Apple|Microsoft|Google|Amazon|Facebook|Tesla|SpaceX)\\b'\n",
    "            ],\n",
    "            'GPE': [\n",
    "                r'\\b(?:New York|Los Angeles|Chicago|Houston|Phoenix|Philadelphia|San Antonio|San Diego|Dallas|San Jose)\\b',\n",
    "                r'\\b(?:United States|America|China|Japan|Germany|France|Italy|Spain|Canada|Australia)\\b'\n",
    "            ],\n",
    "            'DATE': [\n",
    "                r'\\b\\d{1,2}/\\d{1,2}/\\d{4}\\b',  # MM/DD/YYYY\n",
    "                r'\\b(?:January|February|March|April|May|June|July|August|September|October|November|December) \\d{1,2}, \\d{4}\\b',\n",
    "                r'\\b\\d{4}\\b'  # Year\n",
    "            ],\n",
    "            'MONEY': [\n",
    "                r'\\$[\\d,]+(?:\\.\\d{2})?\\b',  # $1,000.00\n",
    "                r'\\b\\d+(?:,\\d{3})* dollars?\\b'\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "    def extract_entities(self, text):\n",
    "        \"\"\"Extract entities using pattern matching\"\"\"\n",
    "        entities = []\n",
    "        \n",
    "        for entity_type, patterns in self.patterns.items():\n",
    "            for pattern in patterns:\n",
    "                matches = re.finditer(pattern, text)\n",
    "                for match in matches:\n",
    "                    entities.append({\n",
    "                        'text': match.group(),\n",
    "                        'label': entity_type,\n",
    "                        'start': match.start(),\n",
    "                        'end': match.end(),\n",
    "                        'confidence': 0.8  # Rule-based confidence\n",
    "                    })\n",
    "        \n",
    "        # Remove duplicates and sort by position\n",
    "        entities = self._remove_duplicates(entities)\n",
    "        entities.sort(key=lambda x: x['start'])\n",
    "        \n",
    "        return entities\n",
    "    \n",
    "    def _remove_duplicates(self, entities):\n",
    "        \"\"\"Remove overlapping entities\"\"\"\n",
    "        unique_entities = []\n",
    "        for entity in entities:\n",
    "            # Check if this entity overlaps with any existing entity\n",
    "            overlaps = False\n",
    "            for existing in unique_entities:\n",
    "                if (entity['start'] < existing['end'] and \n",
    "                    entity['end'] > existing['start']):\n",
    "                    overlaps = True\n",
    "                    break\n",
    "            \n",
    "            if not overlaps:\n",
    "                unique_entities.append(entity)\n",
    "        \n",
    "        return unique_entities\n",
    "\n",
    "# Test the simple NER system\n",
    "simple_ner = SimpleNER()\n",
    "\n",
    "print(\"Simple Rule-based NER Results:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, text in enumerate(sample_texts[:4], 1):\n",
    "    print(f\"\\nText {i}: {text}\")\n",
    "    entities = simple_ner.extract_entities(text)\n",
    "    \n",
    "    if entities:\n",
    "        print(\"Entities found:\")\n",
    "        for ent in entities:\n",
    "            print(f\"  {ent['text']:>20} -> {ent['label']:<8} (confidence: {ent['confidence']})\")\n",
    "    else:\n",
    "        print(\"  No entities found.\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5a627d",
   "metadata": {},
   "source": [
    "## Entity Analysis and Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97afd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze entities across all texts\n",
    "all_entities = []\n",
    "entity_counts = Counter()\n",
    "entity_types_count = Counter()\n",
    "\n",
    "for text in sample_texts:\n",
    "    entities = simple_ner.extract_entities(text)\n",
    "    all_entities.extend(entities)\n",
    "    \n",
    "    for entity in entities:\n",
    "        entity_counts[entity['text']] += 1\n",
    "        entity_types_count[entity['label']] += 1\n",
    "\n",
    "print(\"Entity Statistics:\")\n",
    "print(\"=\" * 30)\n",
    "print(f\"Total entities found: {len(all_entities)}\")\n",
    "print(f\"Unique entities: {len(entity_counts)}\")\n",
    "\n",
    "print(\"\\nEntity types distribution:\")\n",
    "for entity_type, count in entity_types_count.most_common():\n",
    "    print(f\"{entity_type:>8}: {count}\")\n",
    "\n",
    "print(\"\\nMost frequent entities:\")\n",
    "for entity, count in entity_counts.most_common(10):\n",
    "    if count > 1:\n",
    "        print(f\"{entity:>15}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03583b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize entity distribution\n",
    "if entity_types_count:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Entity types distribution\n",
    "    plt.subplot(1, 2, 1)\n",
    "    entity_types = list(entity_types_count.keys())\n",
    "    counts = list(entity_types_count.values())\n",
    "    \n",
    "    plt.bar(entity_types, counts, color='skyblue', alpha=0.7)\n",
    "    plt.title('Entity Types Distribution')\n",
    "    plt.xlabel('Entity Type')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, count in enumerate(counts):\n",
    "        plt.text(i, count + 0.1, str(count), ha='center', va='bottom')\n",
    "    \n",
    "    # Entity length distribution\n",
    "    plt.subplot(1, 2, 2)\n",
    "    entity_lengths = [len(entity['text'].split()) for entity in all_entities]\n",
    "    plt.hist(entity_lengths, bins=range(1, max(entity_lengths) + 2), \n",
    "             color='lightcoral', alpha=0.7, edgecolor='black')\n",
    "    plt.title('Entity Length Distribution (Words)')\n",
    "    plt.xlabel('Number of Words')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No entities found for visualization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a31f99",
   "metadata": {},
   "source": [
    "## Building a Simple ML-based NER\n",
    "Creating a basic machine learning approach for NER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405afc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training data for a simple binary classifier (is entity or not)\n",
    "def create_training_data():\n",
    "    \"\"\"\n",
    "    Create simple training data for entity detection.\n",
    "    \"\"\"\n",
    "    # Known entities and their types\n",
    "    entities = {\n",
    "        'Apple': 'ORG',\n",
    "        'Microsoft': 'ORG',\n",
    "        'Google': 'ORG',\n",
    "        'Barack Obama': 'PERSON',\n",
    "        'Bill Gates': 'PERSON',\n",
    "        'New York': 'GPE',\n",
    "        'California': 'GPE',\n",
    "        'United States': 'GPE',\n",
    "        '2009': 'DATE',\n",
    "        '1975': 'DATE'\n",
    "    }\n",
    "    \n",
    "    # Non-entities\n",
    "    non_entities = [\n",
    "        'the', 'is', 'was', 'planning', 'open', 'store', 'month',\n",
    "        'President', 'founded', 'headquarters', 'located', 'released'\n",
    "    ]\n",
    "    \n",
    "    # Create features and labels\n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    # Add entity examples\n",
    "    for entity in entities.keys():\n",
    "        features.append(entity)\n",
    "        labels.append(1)  # Is entity\n",
    "    \n",
    "    # Add non-entity examples\n",
    "    for word in non_entities:\n",
    "        features.append(word)\n",
    "        labels.append(0)  # Not entity\n",
    "    \n",
    "    return features, labels\n",
    "\n",
    "# Create feature extractor\n",
    "def extract_word_features(word):\n",
    "    \"\"\"\n",
    "    Extract features for a word that might help identify entities.\n",
    "    \"\"\"\n",
    "    features = {\n",
    "        'word': word.lower(),\n",
    "        'is_capitalized': word[0].isupper(),\n",
    "        'is_all_caps': word.isupper(),\n",
    "        'length': len(word),\n",
    "        'has_digit': any(char.isdigit() for char in word),\n",
    "        'starts_with_capital': word[0].isupper() if word else False\n",
    "    }\n",
    "    return features\n",
    "\n",
    "# Get training data\n",
    "train_words, train_labels = create_training_data()\n",
    "\n",
    "# Extract features\n",
    "feature_dicts = [extract_word_features(word) for word in train_words]\n",
    "\n",
    "# Convert to format suitable for scikit-learn\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "vectorizer = DictVectorizer()\n",
    "X_train = vectorizer.fit_transform(feature_dicts)\n",
    "y_train = train_labels\n",
    "\n",
    "# Train a simple classifier\n",
    "classifier = LogisticRegression(random_state=42)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "print(\"Simple ML-based Entity Detector trained!\")\n",
    "print(f\"Training accuracy: {classifier.score(X_train, y_train):.3f}\")\n",
    "print(f\"Number of features: {X_train.shape[1]}\")\n",
    "\n",
    "# Test the classifier\n",
    "test_words = ['Apple', 'the', 'Obama', 'store', 'Tesla', 'building']\n",
    "\n",
    "print(\"\\nTesting ML Entity Detector:\")\n",
    "for word in test_words:\n",
    "    word_features = extract_word_features(word)\n",
    "    X_test = vectorizer.transform([word_features])\n",
    "    prediction = classifier.predict(X_test)[0]\n",
    "    probability = classifier.predict_proba(X_test)[0][1]  # Probability of being entity\n",
    "    \n",
    "    result = \"ENTITY\" if prediction == 1 else \"NOT ENTITY\"\n",
    "    print(f\"{word:>10}: {result:<12} (confidence: {probability:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7771b6",
   "metadata": {},
   "source": [
    "## Text Visualization with Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de93af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_entities_text(text, entities):\n",
    "    \"\"\"\n",
    "    Create a simple text visualization with highlighted entities.\n",
    "    \"\"\"\n",
    "    # Sort entities by start position\n",
    "    entities = sorted(entities, key=lambda x: x['start'])\n",
    "    \n",
    "    # Create highlighted text\n",
    "    highlighted_text = \"\"\n",
    "    last_end = 0\n",
    "    \n",
    "    for entity in entities:\n",
    "        # Add text before entity\n",
    "        highlighted_text += text[last_end:entity['start']]\n",
    "        \n",
    "        # Add highlighted entity\n",
    "        highlighted_text += f\"**{entity['text']}** ({entity['label']})\"\n",
    "        \n",
    "        last_end = entity['end']\n",
    "    \n",
    "    # Add remaining text\n",
    "    highlighted_text += text[last_end:]\n",
    "    \n",
    "    return highlighted_text\n",
    "\n",
    "# Visualize entities in sample texts\n",
    "print(\"Entity Visualization:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, text in enumerate(sample_texts[:3], 1):\n",
    "    entities = simple_ner.extract_entities(text)\n",
    "    highlighted = visualize_entities_text(text, entities)\n",
    "    \n",
    "    print(f\"\\nText {i}:\")\n",
    "    print(f\"Original: {text}\")\n",
    "    print(f\"With entities: {highlighted}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9778ec7d",
   "metadata": {},
   "source": [
    "## Entity Extraction Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec24a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERPipeline:\n",
    "    \"\"\"\n",
    "    Complete NER pipeline combining different approaches.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.rule_based_ner = SimpleNER()\n",
    "        self.spacy_available = SPACY_MODEL_LOADED if 'SPACY_MODEL_LOADED' in globals() else False\n",
    "        \n",
    "        if self.spacy_available:\n",
    "            self.nlp = nlp  # Use the loaded spaCy model\n",
    "    \n",
    "    def extract_entities(self, text, method='rule_based'):\n",
    "        \"\"\"\n",
    "        Extract entities using specified method.\n",
    "        \n",
    "        Args:\n",
    "            text (str): Input text\n",
    "            method (str): 'rule_based', 'spacy', or 'combined'\n",
    "        \n",
    "        Returns:\n",
    "            list: List of entity dictionaries\n",
    "        \"\"\"\n",
    "        if method == 'rule_based':\n",
    "            return self.rule_based_ner.extract_entities(text)\n",
    "        \n",
    "        elif method == 'spacy' and self.spacy_available:\n",
    "            return extract_entities_spacy(text)\n",
    "        \n",
    "        elif method == 'combined':\n",
    "            # Combine rule-based and spaCy results\n",
    "            entities = self.rule_based_ner.extract_entities(text)\n",
    "            \n",
    "            if self.spacy_available:\n",
    "                spacy_entities = extract_entities_spacy(text)\n",
    "                # Simple combination - add spaCy entities that don't overlap\n",
    "                for spacy_ent in spacy_entities:\n",
    "                    overlaps = False\n",
    "                    for rule_ent in entities:\n",
    "                        if (spacy_ent['start'] < rule_ent['end'] and \n",
    "                            spacy_ent['end'] > rule_ent['start']):\n",
    "                            overlaps = True\n",
    "                            break\n",
    "                    if not overlaps:\n",
    "                        entities.append(spacy_ent)\n",
    "            \n",
    "            return sorted(entities, key=lambda x: x['start'])\n",
    "        \n",
    "        else:\n",
    "            return self.rule_based_ner.extract_entities(text)\n",
    "    \n",
    "    def analyze_text(self, text):\n",
    "        \"\"\"\n",
    "        Comprehensive analysis of text entities.\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        # Extract using all available methods\n",
    "        results['rule_based'] = self.extract_entities(text, 'rule_based')\n",
    "        \n",
    "        if self.spacy_available:\n",
    "            results['spacy'] = self.extract_entities(text, 'spacy')\n",
    "            results['combined'] = self.extract_entities(text, 'combined')\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Test the pipeline\n",
    "pipeline = NERPipeline()\n",
    "\n",
    "test_text = \"Apple Inc. was founded by Steve Jobs in Cupertino, California on April 1, 1976.\"\n",
    "results = pipeline.analyze_text(test_text)\n",
    "\n",
    "print(\"NER Pipeline Results:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Text: {test_text}\")\n",
    "print()\n",
    "\n",
    "for method, entities in results.items():\n",
    "    print(f\"{method.upper()} Method:\")\n",
    "    if entities:\n",
    "        for ent in entities:\n",
    "            print(f\"  {ent['text']:>15} -> {ent['label']}\")\n",
    "    else:\n",
    "        print(\"  No entities found.\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c5df6e",
   "metadata": {},
   "source": [
    "## Interactive NER Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e30dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_ner_demo():\n",
    "    \"\"\"\n",
    "    Interactive demo for testing NER on custom text.\n",
    "    \"\"\"\n",
    "    print(\"Interactive NER Demo\")\n",
    "    print(\"=\" * 30)\n",
    "    print(\"Enter text to analyze (or 'quit' to stop):\")\n",
    "    \n",
    "    while True:\n",
    "        user_text = input(\"\\nEnter text: \")\n",
    "        \n",
    "        if user_text.lower() == 'quit':\n",
    "            print(\"Thanks for using the NER demo!\")\n",
    "            break\n",
    "        \n",
    "        if user_text.strip():\n",
    "            entities = pipeline.extract_entities(user_text, 'rule_based')\n",
    "            \n",
    "            if entities:\n",
    "                print(\"\\nEntities found:\")\n",
    "                for ent in entities:\n",
    "                    print(f\"  {ent['text']:>20} -> {ent['label']}\")\n",
    "                \n",
    "                # Show visualization\n",
    "                highlighted = visualize_entities_text(user_text, entities)\n",
    "                print(f\"\\nHighlighted: {highlighted}\")\n",
    "            else:\n",
    "                print(\"\\nNo entities found.\")\n",
    "        else:\n",
    "            print(\"Please enter some text.\")\n",
    "\n",
    "# For demonstration, test with predefined examples\n",
    "demo_texts = [\n",
    "    \"Elon Musk announced that Tesla will open a new factory in Austin, Texas.\",\n",
    "    \"The meeting is scheduled for January 15, 2024 at Microsoft headquarters.\",\n",
    "    \"Amazon reported revenue of $469 billion in 2021.\"\n",
    "]\n",
    "\n",
    "print(\"Demo NER Results:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "for i, text in enumerate(demo_texts, 1):\n",
    "    print(f\"\\nExample {i}: {text}\")\n",
    "    entities = pipeline.extract_entities(text, 'rule_based')\n",
    "    \n",
    "    if entities:\n",
    "        print(\"Entities:\")\n",
    "        for ent in entities:\n",
    "            print(f\"  {ent['text']:>20} -> {ent['label']}\")\n",
    "        \n",
    "        highlighted = visualize_entities_text(text, entities)\n",
    "        print(f\"Highlighted: {highlighted}\")\n",
    "    else:\n",
    "        print(\"No entities found.\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Uncomment to run interactive demo\n",
    "# interactive_ner_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c432d55e",
   "metadata": {},
   "source": [
    "## Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf86ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple evaluation of our NER system\n",
    "def evaluate_ner_simple():\n",
    "    \"\"\"\n",
    "    Simple evaluation based on known entities in our test sentences.\n",
    "    \"\"\"\n",
    "    # Ground truth entities for first few sample texts\n",
    "    ground_truth = {\n",
    "        0: [('Apple Inc.', 'ORG'), ('New York City', 'GPE')],\n",
    "        1: [('Barack Obama', 'PERSON'), ('United States', 'GPE'), ('2009', 'DATE'), ('2017', 'DATE')],\n",
    "        2: [('Microsoft Corporation', 'ORG'), ('Bill Gates', 'PERSON'), ('Paul Allen', 'PERSON'), ('1975', 'DATE')]\n",
    "    }\n",
    "    \n",
    "    total_true = 0\n",
    "    total_predicted = 0\n",
    "    correct_predictions = 0\n",
    "    \n",
    "    for idx, true_entities in ground_truth.items():\n",
    "        text = sample_texts[idx]\n",
    "        predicted_entities = pipeline.extract_entities(text, 'rule_based')\n",
    "        \n",
    "        # Convert to comparable format\n",
    "        true_set = set(true_entities)\n",
    "        pred_set = set([(ent['text'], ent['label']) for ent in predicted_entities])\n",
    "        \n",
    "        total_true += len(true_set)\n",
    "        total_predicted += len(pred_set)\n",
    "        correct_predictions += len(true_set & pred_set)\n",
    "        \n",
    "        print(f\"Text {idx + 1}:\")\n",
    "        print(f\"  True: {true_set}\")\n",
    "        print(f\"  Predicted: {pred_set}\")\n",
    "        print(f\"  Correct: {true_set & pred_set}\")\n",
    "        print()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precision = correct_predictions / total_predicted if total_predicted > 0 else 0\n",
    "    recall = correct_predictions / total_true if total_true > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    print(\"Evaluation Results:\")\n",
    "    print(f\"Precision: {precision:.3f}\")\n",
    "    print(f\"Recall: {recall:.3f}\")\n",
    "    print(f\"F1-Score: {f1:.3f}\")\n",
    "    \n",
    "    return precision, recall, f1\n",
    "\n",
    "# Run evaluation\n",
    "precision, recall, f1 = evaluate_ner_simple()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398b83f2",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **NER identifies and classifies** named entities in text into predefined categories\n",
    "2. **Rule-based approaches** use patterns and dictionaries, good for specific domains\n",
    "3. **Machine learning approaches** can generalize better but need training data\n",
    "4. **Pre-trained models** like spaCy work well out-of-the-box for general domains\n",
    "5. **Evaluation is important** - use precision, recall, and F1-score\n",
    "6. **Combining approaches** can improve overall performance\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Try more sophisticated ML models (CRF, BiLSTM-CRF)\n",
    "- Use transformer-based models (BERT for NER)\n",
    "- Create domain-specific NER systems\n",
    "- Handle nested and overlapping entities\n",
    "- Build entity linking systems (connecting entities to knowledge bases)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
